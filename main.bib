@article{DBLP:journals/x/Turing37,
  author    = {Alan M. Turing},
  title     = {On computable numbers, with an application to the Entscheidungsproblem},
  journal   = {Proc. London Math. Soc.},
  volume    = {s2-42},
  number    = {1},
  pages     = {230--265},
  year      = {1937}
}
@Article{liu2022bit,
  author        = {Zechun Liu and Barlas Oguz and Aasish Pappu and Lin Xiao and Scott Yih and Meng Li and Raghuraman Krishnamoorthi and Yashar Mehdad},
  title         = {BiT: Robustly Binarized Multi-distilled Transformer},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2205.13016},
  file          = {:C\:/Users/f50038439/Documents/Papers/BiT - Robustly Binarized Multi-distilled Transformer.pdf:PDF},
  groups        = {Quantization, Distillation},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
  readstatus    = {read},
}

@Article{liu2023ecoformer,
  author        = {Jing Liu and Zizheng Pan and Haoyu He and Jianfei Cai and Bohan Zhuang},
  title         = {EcoFormer: Energy-Saving Attention with Linear Complexity},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2209.09004},
  file          = {:C\:/Users/f50038439/Documents/Papers/EcoFormer.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{vaswani2023attention,
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title         = {Attention Is All You Need},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {1706.03762},
  file          = {:C\:/Users/f50038439/Documents/Papers/Attention Is All You Need.pdf:PDF},
  primaryclass  = {cs.CL},
  ranking       = {rank5},
  readstatus    = {read},
}

@Misc{lin2021survey,
  author        = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
  title         = {A Survey of Transformers},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2106.04554},
  file          = {:C\:/Users/f50038439/Documents/Papers/A Survey on Transformers.pdf:PDF},
  groups        = {Surveys},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{peng2021random,
  author        = {Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah A. Smith and Lingpeng Kong},
  title         = {Random Feature Attention},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2103.02143},
  file          = {:C\:/Users/f50038439/Documents/Papers/Random Feature Attention.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.CL},
  readstatus    = {read},
}

@Article{choromanski2022rethinking,
  author        = {Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
  title         = {Rethinking Attention with Performers},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2009.14794},
  file          = {:C\:/Users/f50038439/Documents/Papers/Rethinking Attention with Performers.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{kitaev2020reformer,
  author        = {Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
  title         = {Reformer: The Efficient Transformer},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2001.04451},
  file          = {:C\:/Users/f50038439/Documents/Papers/Reformer The Efficient Transformer.pdf:PDF},
  groups        = {Sparse Attention},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Khan_2022,
  author     = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal    = {ACM Computing Surveys},
  title      = {Transformers in Vision: A Survey},
  year       = {2022},
  issn       = {1557-7341},
  month      = jan,
  number     = {10s},
  pages      = {1–41},
  volume     = {54},
  doi        = {10.1145/3505244},
  file       = {:C\:/Users/f50038439/Documents/Papers/Transformers in Vision A Survey.pdf:PDF},
  groups     = {Surveys},
  publisher  = {Association for Computing Machinery (ACM)},
  readstatus = {skimmed},
  url        = {http://dx.doi.org/10.1145/3505244},
}

@Misc{schlag2021linear,
  author        = {Imanol Schlag and Kazuki Irie and Jürgen Schmidhuber},
  title         = {Linear Transformers Are Secretly Fast Weight Programmers},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2102.11174},
  file          = {:C\:/Users/f50038439/Documents/Papers/Linear Transformers Are Secretly Fast Weight Programmers.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Misc{katharopoulos2020transformers,
  author        = {Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
  title         = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.16236},
  file          = {:C\:/Users/f50038439/Documents/Papers/Fast Autoregressive Transformers with Linear Attention.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Misc{tay2022efficient,
  author        = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
  title         = {Efficient Transformers: A Survey},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2009.06732},
  file          = {:C\:/Users/f50038439/Documents/Papers/Efficient Transformers - A Survey.pdf:PDF},
  groups        = {Surveys},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Misc{nguyen2023boolean,
  author        = {Van Minh Nguyen},
  title         = {Boolean Variation and Boolean Logic BackPropagation},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2311.07427},
  file          = {:C\:/Users/f50038439/Documents/Papers/boolean_variation_logic_backpropagation.pdf:PDF},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Misc{sun2023retentive,
  author        = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
  title         = {Retentive Network: A Successor to Transformer for Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.08621},
  file          = {:C\:/Users/f50038439/Documents/Papers/sun2023retentive - Retentive Network_ a Successor to Transformer for Large Language Models.pdf:PDF},
  primaryclass  = {cs.CL},
  priority      = {prio2},
}

@Misc{alizadeh2024llm,
  author        = {Keivan Alizadeh and Iman Mirzadeh and Dmitry Belenko and Karen Khatamifard and Minsik Cho and Carlo C Del Mundo and Mohammad Rastegari and Mehrdad Farajtabar},
  title         = {LLM in a flash: Efficient Large Language Model Inference with Limited Memory},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2312.11514},
  file          = {:C\:/Users/f50038439/Documents/Papers/LLM in a flash.pdf:PDF},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.11514},
}

@Misc{zhang2023binarized,
  author        = {Yichi Zhang and Ankush Garg and Yuan Cao and Łukasz Lew and Behrooz Ghorbani and Zhiru Zhang and Orhan Firat},
  title         = {Binarized Neural Machine Translation},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2302.04907},
  file          = {:Papers/Binarized Neural Machine Translation.pdf:PDF},
  groups        = {Quantization},
  primaryclass  = {cs.CL},
  readstatus    = {skimmed},
}

@Misc{He2023,
  author        = {Yefei He and Zhenyu Lou and Luoming Zhang and Jing Liu and Weijia Wu and Hong Zhou and Bohan Zhuang},
  title         = {BiViT: Extremely Compressed Binary Vision Transformer},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2211.07091},
  file          = {:Papers/BiViT.pdf:PDF},
  groups        = {Quantization},
  primaryclass  = {cs.CV},
  readstatus    = {skimmed},
}

@Misc{He2023a,
  author        = {Bobby He and Thomas Hofmann},
  title         = {Simplifying Transformer Blocks},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2311.01906},
  primaryclass  = {cs.LG},
}

@Misc{Brody2023,
  author        = {Shaked Brody and Uri Alon and Eran Yahav},
  title         = {On the Expressivity Role of LayerNorm in Transformers' Attention},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2305.02582},
  file          = {:Papers/On the Expressivity Role of LayerNorm in Transformers' Attention.pdf:PDF},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Misc{Le2023,
  author        = {Phuoc-Hoan Charles Le and Xinlin Li},
  title         = {BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2306.16678},
  groups        = {Quantization},
  primaryclass  = {cs.CV},
  priority      = {prio3},
  readstatus    = {skimmed},
}

@Misc{Wang2023,
  author        = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
  title         = {BitNet: Scaling 1-bit Transformers for Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2310.11453},
  groups        = {Quantization},
  primaryclass  = {cs.CL},
  ranking       = {rank4},
  readstatus    = {skimmed},
}

@Misc{Saha2024,
  author        = {Barna Saha and Christopher Ye},
  title         = {The I/O Complexity of Attention, or How Optimal is Flash Attention?},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.07443},
  primaryclass  = {cs.LG},
}

@Misc{Han2023,
  author        = {Insu Han and Rajesh Jayaram and Amin Karbasi and Vahab Mirrokni and David P. Woodruff and Amir Zandieh},
  title         = {HyperAttention: Long-context Attention in Near-Linear Time},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2310.05869},
  groups        = {Sparse Attention},
  primaryclass  = {cs.LG},
  priority      = {prio2},
}

@Misc{Touvron2021,
  author        = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Hervé Jégou},
  title         = {Training data-efficient image transformers & distillation through attention},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2012.12877},
  file          = {:Papers/DeIT.pdf:PDF},
  groups        = {Distillation},
  primaryclass  = {cs.CV},
  readstatus    = {skimmed},
}

@Misc{Tang2024,
  author        = {Yehui Tang and Yunhe Wang and Jianyuan Guo and Zhijun Tu and Kai Han and Hailin Hu and Dacheng Tao},
  title         = {A Survey on Transformer Compression},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.05964},
  groups        = {Surveys},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  readstatus    = {skimmed},
}

@Misc{Zhao2023,
  author        = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  title         = {A Survey of Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2303.18223},
  file          = {:Papers/Survay of LLMs.pdf:PDF},
  groups        = {Surveys},
  primaryclass  = {cs.CL},
  priority      = {prio1},
}

@Misc{Minaee2024,
  author        = {Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
  title         = {Large Language Models: A Survey},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.06196},
  groups        = {Surveys},
  primaryclass  = {cs.CL},
}

@Misc{Yao2022,
  author         = {Zhewei Yao and Reza Yazdani Aminabadi and Minjia Zhang and Xiaoxia Wu and Conglong Li and Yuxiong He},
  title          = {ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  year           = {2022},
  archiveprefix  = {arXiv},
  comment        = {Introduces an efficient and hardware friendly quantization scheme for weight and activation alongisde a lightweight knowledge distillation algorithm.},
  eprint         = {2206.01861},
  file           = {:Papers/ZeroQuant.pdf:PDF},
  groups         = {Quantization},
  primaryclass   = {cs.CL},
  qualityassured = {qualityAssured},
  ranking        = {rank2},
  readstatus     = {skimmed},
  relevance      = {relevant},
}

@Misc{Qin2022,
  author        = {Haotong Qin and Yifu Ding and Mingyuan Zhang and Qinghua Yan and Aishan Liu and Qingqing Dang and Ziwei Liu and Xianglong Liu},
  title         = {BiBERT: Accurate Fully Binarized BERT},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2203.06390},
  file          = {:Papers/BiBERT.pdf:PDF},
  groups        = {Quantization, Distillation},
  primaryclass  = {cs.CL},
  priority      = {prio3},
  ranking       = {rank3},
  readstatus    = {skimmed},
}

@Misc{Peng2023,
  author        = {Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
  title         = {RWKV: Reinventing RNNs for the Transformer Era},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2305.13048},
  file          = {:Papers/RWKV.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.CL},
}

@Misc{Zhai2021,
  author        = {Shuangfei Zhai and Walter Talbott and Nitish Srivastava and Chen Huang and Hanlin Goh and Ruixiang Zhang and Josh Susskind},
  title         = {An Attention Free Transformer},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2105.14103},
  groups        = {Linear Transformers},
  primaryclass  = {cs.LG},
}
@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{luong2015effective,
      title={Effective Approaches to Attention-based Neural Machine Translation}, 
      author={Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
      year={2015},
      eprint={1508.04025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{kim2017structured,
      title={Structured Attention Networks}, 
      author={Yoon Kim and Carl Denton and Luong Hoang and Alexander M. Rush},
      year={2017},
      eprint={1702.00887},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{10.1162/neco.1997.9.8.1735,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}





@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Linear Transformers\;0\;1\;0x0000ffff\;\;\;;
1 StaticGroup:Quantization\;0\;0\;0xffff00ff\;\;\;;
1 StaticGroup:Surveys\;0\;1\;0xcc8033ff\;\;\;;
1 StaticGroup:Sparse Attention\;0\;1\;0x99cc99ff\;\;\;;
1 StaticGroup:Distillation\;0\;1\;0xb31a1aff\;\;\;;
}

