@article{DBLP:journals/x/Turing37,
  author    = {Alan M. Turing},
  title     = {On computable numbers, with an application to the Entscheidungsproblem},
  journal   = {Proc. London Math. Soc.},
  volume    = {s2-42},
  number    = {1},
  pages     = {230--265},
  year      = {1937}
}
@misc{liu2022bit,
  author        = {Zechun Liu and Barlas Oguz and Aasish Pappu and Lin Xiao and Scott Yih and Meng Li and Raghuraman Krishnamoorthi and Yashar Mehdad},
  title         = {BiT: Robustly Binarized Multi-distilled Transformer},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2205.13016},
  file          = {:C\:/Users/f50038439/Documents/Papers/BiT - Robustly Binarized Multi-distilled Transformer.pdf:PDF},
  groups        = {Quantization, Distillation},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
  readstatus    = {read},
}

@misc{liu2023ecoformer,
  author        = {Jing Liu and Zizheng Pan and Haoyu He and Jianfei Cai and Bohan Zhuang},
  title         = {EcoFormer: Energy-Saving Attention with Linear Complexity},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2209.09004},
  file          = {:C\:/Users/f50038439/Documents/Papers/EcoFormer.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@misc{vaswani2023attention,
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title         = {Attention Is All You Need},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {1706.03762},
  file          = {:C\:/Users/f50038439/Documents/Papers/Attention Is All You Need.pdf:PDF},
  primaryclass  = {cs.CL},
  ranking       = {rank5},
  readstatus    = {read},
}
@misc{rastegari2016xnornet,
      title={XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}, 
      author={Mohammad Rastegari and Vicente Ordonez and Joseph Redmon and Ali Farhadi},
      year={2016},
      eprint={1603.05279},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{qin2020forward,
      title={Forward and Backward Information Retention for Accurate Binary Neural Networks}, 
      author={Haotong Qin and Ruihao Gong and Xianglong Liu and Mingzhu Shen and Ziran Wei and Fengwei Yu and Jingkuan Song},
      year={2020},
      eprint={1909.10788},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{zhang2020ternarybert,
      title={TernaryBERT: Distillation-aware Ultra-low Bit BERT}, 
      author={Wei Zhang and Lu Hou and Yichun Yin and Lifeng Shang and Xiao Chen and Xin Jiang and Qun Liu},
      year={2020},
      eprint={2009.12812},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{bai2021binarybert,
      title={BinaryBERT: Pushing the Limit of BERT Quantization}, 
      author={Haoli Bai and Wei Zhang and Lu Hou and Lifeng Shang and Jing Jin and Xin Jiang and Qun Liu and Michael Lyu and Irwin King},
      year={2021},
      eprint={2012.15701},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{shen2019qbert,
      title={Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT}, 
      author={Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
      year={2019},
      eprint={1909.05840},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@Misc{lin2021survey,
  author        = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
  title         = {A Survey of Transformers},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2106.04554},
  file          = {:C\:/Users/f50038439/Documents/Papers/A Survey on Transformers.pdf:PDF},
  groups        = {Surveys},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@misc{peng2021random,
  author        = {Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah A. Smith and Lingpeng Kong},
  title         = {Random Feature Attention},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2103.02143},
  file          = {:C\:/Users/f50038439/Documents/Papers/Random Feature Attention.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.CL},
  readstatus    = {read},
}

@misc{choromanski2022rethinking,
  author        = {Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
  title         = {Rethinking Attention with Performers},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2009.14794},
  file          = {:C\:/Users/f50038439/Documents/Papers/Rethinking Attention with Performers.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@misc{kitaev2020reformer,
  author        = {Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
  title         = {Reformer: The Efficient Transformer},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2001.04451},
  file          = {:C\:/Users/f50038439/Documents/Papers/Reformer The Efficient Transformer.pdf:PDF},
  groups        = {Sparse Attention},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Khan_2022,
  author     = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal    = {ACM Computing Surveys},
  title      = {Transformers in Vision: A Survey},
  year       = {2022},
  issn       = {1557-7341},
  month      = jan,
  number     = {10s},
  pages      = {1–41},
  volume     = {54},
  doi        = {10.1145/3505244},
  file       = {:C\:/Users/f50038439/Documents/Papers/Transformers in Vision A Survey.pdf:PDF},
  groups     = {Surveys},
  publisher  = {Association for Computing Machinery (ACM)},
  readstatus = {skimmed},
  url        = {http://dx.doi.org/10.1145/3505244},
}

@Misc{schlag2021linear,
  author        = {Imanol Schlag and Kazuki Irie and Jürgen Schmidhuber},
  title         = {Linear Transformers Are Secretly Fast Weight Programmers},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2102.11174},
  file          = {:C\:/Users/f50038439/Documents/Papers/Linear Transformers Are Secretly Fast Weight Programmers.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Misc{katharopoulos2020transformers,
  author        = {Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
  title         = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.16236},
  file          = {:C\:/Users/f50038439/Documents/Papers/Fast Autoregressive Transformers with Linear Attention.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Misc{tay2022efficient,
  author        = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
  title         = {Efficient Transformers: A Survey},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2009.06732},
  file          = {:C\:/Users/f50038439/Documents/Papers/Efficient Transformers - A Survey.pdf:PDF},
  groups        = {Surveys},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Misc{nguyen2023boolean,
  author        = {Van Minh Nguyen},
  title         = {Boolean Variation and Boolean Logic BackPropagation},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2311.07427},
  file          = {:C\:/Users/f50038439/Documents/Papers/boolean_variation_logic_backpropagation.pdf:PDF},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Misc{sun2023retentive,
  author        = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
  title         = {Retentive Network: A Successor to Transformer for Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.08621},
  file          = {:C\:/Users/f50038439/Documents/Papers/sun2023retentive - Retentive Network_ a Successor to Transformer for Large Language Models.pdf:PDF},
  primaryclass  = {cs.CL},
  priority      = {prio2},
}

@Misc{alizadeh2024llm,
  author        = {Keivan Alizadeh and Iman Mirzadeh and Dmitry Belenko and Karen Khatamifard and Minsik Cho and Carlo C Del Mundo and Mohammad Rastegari and Mehrdad Farajtabar},
  title         = {LLM in a flash: Efficient Large Language Model Inference with Limited Memory},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2312.11514},
  file          = {:C\:/Users/f50038439/Documents/Papers/LLM in a flash.pdf:PDF},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.11514},
}

@Misc{zhang2023binarized,
  author        = {Yichi Zhang and Ankush Garg and Yuan Cao and Łukasz Lew and Behrooz Ghorbani and Zhiru Zhang and Orhan Firat},
  title         = {Binarized Neural Machine Translation},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2302.04907},
  file          = {:Papers/Binarized Neural Machine Translation.pdf:PDF},
  groups        = {Quantization},
  primaryclass  = {cs.CL},
  readstatus    = {skimmed},
}

@Misc{He2023,
  author        = {Yefei He and Zhenyu Lou and Luoming Zhang and Jing Liu and Weijia Wu and Hong Zhou and Bohan Zhuang},
  title         = {BiViT: Extremely Compressed Binary Vision Transformer},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2211.07091},
  file          = {:Papers/BiViT.pdf:PDF},
  groups        = {Quantization},
  primaryclass  = {cs.CV},
  readstatus    = {skimmed},
}

@Misc{He2023a,
  author        = {Bobby He and Thomas Hofmann},
  title         = {Simplifying Transformer Blocks},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2311.01906},
  primaryclass  = {cs.LG},
}

@Misc{Brody2023,
  author        = {Shaked Brody and Uri Alon and Eran Yahav},
  title         = {On the Expressivity Role of LayerNorm in Transformers' Attention},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2305.02582},
  file          = {:Papers/On the Expressivity Role of LayerNorm in Transformers' Attention.pdf:PDF},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Misc{Le2023,
  author        = {Phuoc-Hoan Charles Le and Xinlin Li},
  title         = {BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2306.16678},
  groups        = {Quantization},
  primaryclass  = {cs.CV},
  priority      = {prio3},
  readstatus    = {skimmed},
}

@Misc{Wang2023,
  author        = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
  title         = {BitNet: Scaling 1-bit Transformers for Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2310.11453},
  groups        = {Quantization},
  primaryclass  = {cs.CL},
  ranking       = {rank4},
  readstatus    = {skimmed},
}

@Misc{Saha2024,
  author        = {Barna Saha and Christopher Ye},
  title         = {The I/O Complexity of Attention, or How Optimal is Flash Attention?},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.07443},
  primaryclass  = {cs.LG},
}

@Misc{Han2023,
  author        = {Insu Han and Rajesh Jayaram and Amin Karbasi and Vahab Mirrokni and David P. Woodruff and Amir Zandieh},
  title         = {HyperAttention: Long-context Attention in Near-Linear Time},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2310.05869},
  groups        = {Sparse Attention},
  primaryclass  = {cs.LG},
  priority      = {prio2},
}

@Misc{Touvron2021,
  author        = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Hervé Jégou},
  title         = {Training data-efficient image transformers & distillation through attention},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2012.12877},
  file          = {:Papers/DeIT.pdf:PDF},
  groups        = {Distillation},
  primaryclass  = {cs.CV},
  readstatus    = {skimmed},
}

@Misc{Tang2024,
  author        = {Yehui Tang and Yunhe Wang and Jianyuan Guo and Zhijun Tu and Kai Han and Hailin Hu and Dacheng Tao},
  title         = {A Survey on Transformer Compression},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.05964},
  groups        = {Surveys},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  readstatus    = {skimmed},
}

@Misc{Zhao2023,
  author        = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  title         = {A Survey of Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2303.18223},
  file          = {:Papers/Survay of LLMs.pdf:PDF},
  groups        = {Surveys},
  primaryclass  = {cs.CL},
  priority      = {prio1},
}

@Misc{Minaee2024,
  author        = {Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
  title         = {Large Language Models: A Survey},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.06196},
  groups        = {Surveys},
  primaryclass  = {cs.CL},
}

@Misc{Yao2022,
  author         = {Zhewei Yao and Reza Yazdani Aminabadi and Minjia Zhang and Xiaoxia Wu and Conglong Li and Yuxiong He},
  title          = {ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  year           = {2022},
  archiveprefix  = {arXiv},
  comment        = {Introduces an efficient and hardware friendly quantization scheme for weight and activation alongisde a lightweight knowledge distillation algorithm.},
  eprint         = {2206.01861},
  file           = {:Papers/ZeroQuant.pdf:PDF},
  groups         = {Quantization},
  primaryclass   = {cs.CL},
  qualityassured = {qualityAssured},
  ranking        = {rank2},
  readstatus     = {skimmed},
  relevance      = {relevant},
}

@Misc{Qin2022bibert,
  author        = {Haotong Qin and Yifu Ding and Mingyuan Zhang and Qinghua Yan and Aishan Liu and Qingqing Dang and Ziwei Liu and Xianglong Liu},
  title         = {BiBERT: Accurate Fully Binarized BERT},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2203.06390},
  file          = {:Papers/BiBERT.pdf:PDF},
  groups        = {Quantization, Distillation},
  primaryclass  = {cs.CL},
  priority      = {prio3},
  ranking       = {rank3},
  readstatus    = {skimmed},
}

@Misc{Peng2023,
  author        = {Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
  title         = {RWKV: Reinventing RNNs for the Transformer Era},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2305.13048},
  file          = {:Papers/RWKV.pdf:PDF},
  groups        = {Linear Transformers},
  primaryclass  = {cs.CL},
}

@Misc{Zhai2021,
  author        = {Shuangfei Zhai and Walter Talbott and Nitish Srivastava and Chen Huang and Hanlin Goh and Ruixiang Zhang and Josh Susskind},
  title         = {An Attention Free Transformer},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2105.14103},
  groups        = {Linear Transformers},
  primaryclass  = {cs.LG},
}
@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{luong2015effective,
      title={Effective Approaches to Attention-based Neural Machine Translation}, 
      author={Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
      year={2015},
      eprint={1508.04025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{kim2017structured,
      title={Structured Attention Networks}, 
      author={Yoon Kim and Carl Denton and Luong Hoang and Alexander M. Rush},
      year={2017},
      eprint={1702.00887},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yu2016orthogonal,
      title={Orthogonal Random Features}, 
      author={Felix X. Yu and Ananda Theertha Suresh and Krzysztof Choromanski and Daniel Holtmann-Rice and Sanjiv Kumar},
      year={2016},
      eprint={1610.09072},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{clevert2016elu,
      title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}, 
      author={Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
      year={2016},
      eprint={1511.07289},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{10.1162/neco.1997.9.8.1735,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}
@InProceedings{bojar-EtAl:2014:W14-33,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale
{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}@misc{tay2020long,
      title={Long Range Arena: A Benchmark for Efficient Transformers}, 
      author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
      year={2020},
      eprint={2011.04006},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{bengio2013estimating,
      title={Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation}, 
      author={Yoshua Bengio and Nicholas Léonard and Aaron Courville},
      year={2013},
      eprint={1308.3432},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{gholami2021surveyquant,
      title={A Survey of Quantization Methods for Efficient Neural Network Inference}, 
      author={Amir Gholami and Sehoon Kim and Zhen Dong and Zhewei Yao and Michael W. Mahoney and Kurt Keutzer},
      year={2021},
      eprint={2103.13630},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{liu2020reactnet,
  title={ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions},
  author={Liu, Zechun and Shen, Zhiqiang and Savvides, Marios and Cheng, Kwang-Ting},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2020}
}
@misc{wang2019glue,
     title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
     author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
     note={In the Proceedings of ICLR.},
     year={2019}
 }
@misc{huang2024billm,
      title={BiLLM: Pushing the Limit of Post-Training Quantization for LLMs}, 
      author={Wei Huang and Yangdong Liu and Haotong Qin and Ying Li and Shiming Zhang and Xianglong Liu and Michele Magno and Xiaojuan Qi},
      year={2024},
      eprint={2402.04291},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{leimao2023optiminference,
    title={Transformer Autoregressive Inference Optimization},
    author={Lei Mao},
    year={2023},
    howpublished={\url{https://leimao.github.io/article/Transformer-Autoregressive-Inference-Optimization/}},
}
@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{jacob2017quantization,
      title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}, 
      author={Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
      year={2017},
      eprint={1712.05877},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@INPROCEEDINGS{energy7929192,
  author={Nurvitadhi, Eriko and Sheffield, David and Jaewoong Sim and Mishra, Asit and Venkatesh, Ganesh and Marr, Debbie},
  booktitle={2016 International Conference on Field-Programmable Technology (FPT)}, 
  title={Accelerating Binarized Neural Networks: Comparison of FPGA, CPU, GPU, and ASIC}, 
  year={2016},
  volume={},
  number={},
  pages={77-84},
  keywords={Neurons;Random access memory;Biological neural networks;Field programmable gate arrays;Graphics processing units;Hardware;System-on-chip;Deep learning;binarized neural networks;FPGA;CPU;GPU;ASIC;data analytics;hardware accelerator},
  doi={10.1109/FPT.2016.7929192}}





@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Linear Transformers\;0\;1\;0x0000ffff\;\;\;;
1 StaticGroup:Quantization\;0\;0\;0xffff00ff\;\;\;;
1 StaticGroup:Surveys\;0\;1\;0xcc8033ff\;\;\;;
1 StaticGroup:Sparse Attention\;0\;1\;0x99cc99ff\;\;\;;
1 StaticGroup:Distillation\;0\;1\;0xb31a1aff\;\;\;;
}

