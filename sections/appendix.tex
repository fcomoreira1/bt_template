\section{Transformer Autoregressive Inference Optimization}
\label{sec:appendix}

As discussed in section \ref{sec:transformers}, directly following the Vanilla Transformer for autoregressive inference can be asymptotically very slow. During inference, we feed the first $M$ tokens of the currently generated output sequence to obtain a probability distribution over the next token, which is used to decide on the $M+1$-th token of the sequence. In this fashion, as every pass-through on a decoder block has $O(M^2)$ complexity, we get a total computational complexity of $O(M^3)$, which needless to say is undesirable.

If we compare for instance with recurrent networks, which memorize previous states during inference, we get the idea of reusing previous computations to reduce the asymptotic complexity. We will develop this idea mathematically taking inspiration from \cite{leimao2023optiminference}, by first showing how we can compute a pass of the masked self-attention in $O(M)$. 

Let $X_M$ represent the input of the decoder at time $M$ and let $(x_i)_{i=1}^M$ be its tokens, $x_i \in \mathbb{R}^d$. Then it holds that $X_{M+1} = [X_M^\top, x_{M+1}]^\top$. Also, let 
\begin{equation}
    Q_M = X_M W_Q, \;\; K_M = X_M W_K, \;\; V_M = X_M W_V,\;\; Y_M = A_{masked}(Q_M, K_M, V_M).
\end{equation}

Now we can compute 
\begin{equation}
Q_{M+1} = X_{M+1}W_Q =
\begin{bmatrix}
    X_{M}W_Q \\
    x_{m+1}^\top W_Q
\end{bmatrix}
    = 
    \begin{bmatrix}
    Q_M \\
    x_{m+1}^\top W_Q
\end{bmatrix},
\end{equation}
and analogous results hold for $K_{M+1}, V_{M+1}$. For simplicity, let $q_{M+1}^\top = x_{m+1}^\top W_Q$ and define $k_{M+1}$, $v_{M+1}$ analogously. Let us use this information to rewrite $Y_{M+1}$ as
\begin{equation}
    \begin{split}
        Y_{M+1} &= \softmax(\text{mask}(Q_{M+1} K_{M+1}^\top)) V_{M+1} \\
            &= \softmax(\text{mask}\left(
                \begin{bmatrix}
                    \begin{array}{c|c}
                        Q_M K_M^\top & Q_M k_{M+1} \\ 
                        \hline
                        q_{M+1}^\top K_M^\top & q_{M+1}^\top k_{M+1}
                    \end{array}
                \end{bmatrix}   
            \right)) V_{M+1} \\
            &= \softmax\left(
                \begin{bmatrix}
                    \begin{array}{c|c}
                        \text{mask}(Q_M K_M^\top) & -\infty \\ 
                        \hline
                        q_{M+1}^\top K_M^\top & q_{M+1}^\top k_{M+1}
                    \end{array}
                \end{bmatrix}   
            \right) V_{M+1} \\
            &=
            \softmax\left(
                \begin{bmatrix}
                    \begin{array}{c|c}
                        \softmax(\text{mask}(Q_M K_M^\top) & 0 \\
                        \hline
                    \end{array} \\
                    q_{M+1}^\top K_{M+1}^\top
                \end{bmatrix}   
            \right) \begin{bmatrix}
                V_M \\
                v_{M+1}^\top
            \end{bmatrix}
            \\ 
            &= \begin{bmatrix}
                Y_M \\
                \softmax(q_{M+1}^\top K_{M+1}^\top) V_{M+1}
            \end{bmatrix}.
    \end{split}
\end{equation}

Therefore, having computed $Y_M$ before, we only need to compute the bottom term of the expression above. Moreover, we already have computed $K_M, V_M$, so we first compute $q_{M+1}, k_{M+1}, v_{M+1}$ in $O(dd_k)$ each, then having all the values computed, it remains to compute the matrix multiplications. $q_{M+1}^\top K_{M+1}^\top$ takes $O(Md_k)$, the softmax $O(Md_k)$ and finally, multiplying with $V_{M+1}$ requires $O(Md_k)$ operations too, resulting in a final complexity of $O((M+d)d_k)$ operations to pass through the masked self-attention block. 

Very similarly, we can get that the output of the encoder-decoder attention can be computed in linear time with respect to $M$ having the previous values computed. Finally, as stated in table \ref{table:1}, the complexity of $FFN$ is already linear on $M$, therefore we can do a full pass on a decoder block during inference with $O(M)$ computations, reducing the total requirements of autoregressive inference to $O(M^2)$.y
