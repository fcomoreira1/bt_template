\section{Conclusion}

In this work, we explored the well-established Transformer architecture. With a focus on efficient modifications to the base model, we first introduce the Vanilla Transformer, as introduced by Vaswani et al. alongside some now standard adaptations to its usage, ranging from modifications to the architecture to pre-training practices. We then establish some of the bottlenecks that arose with its wide adoption, notably the high memory and computational complexity of its attention blocks and massive model sizes. Intending to present solutions to remediate these problems, we present two approaches: Binarization and Linear Attention. From two different perspectives, these approaches have empirically improved the efficiency of Transformer-based models. The former focuses on a practical and hardware-centered way of reducing the costs of the networks, while the latter aims to improve the theoretical complexities of Transformer models. Both approaches are very recent, and as discussed in each section separately, are still open for improvements. With new works from the past year displaying promising results, we believe these could be promising directions for future research.