\section{Introduction}

Transformers \cite{vaswani2023attention} was introduced in 2017 and since then has powered many of the recent advancements in Artificial Intelligence (AI). Inspired by the attention mechanism for LSTMs \cite{10.1162/neco.1997.9.8.1735}\cite{bahdanau2016neural}\cite{luong2015effective}\cite{kim2017structured} that was ubiquitous for sequence-to-sequence (seq-to-seq) tasks,  Vaswani et al. proposed a model architecture that removed the recurrent network often associated with it, and relied entirely on attention for the sequence modeling. In doing so, the Transformers architecture deals with one of the fundamental bottlenecks of recurrent networks, the limited parallelization during training due to the sequential nature of dependencies. On the other hand, the Transformer can be efficiently parallelized which allows not only for faster training but also for deeper networks with more representational power.


Transformer-based models have been widely adopted in diverse fields,
such as natural language processing (NLP) and computer vision (CV), achieving state-of-the-art on various tasks. As such, there is a rich variety of research dedicated to applications and fundamental improvements to its original formulation. Alongside clever engineering, the strong scaling ability of Transformer-based models has played a big part in its success, with most of the models with billions of parameters being based on this architecture. Particularly, NLP-related tasks have been dominated by Transformer-based pretrained large language models (LLMs).

These increasingly larger models have demonstrated significant capabilities, however their sheer volume of parameters easily reaching hundreds of billions are associated with enormous computational and memory expenses. That means that without a significant redesign, these models have limited deployability for edge devices such as personal computers and mobile phones. Indeed, if we take the case of LLaMa2-70B model \cite{touvron2023llama}, it requires 150GB of storage in its default 16-bit floating point format, which is much beyond what any edge device realistically can afford to run. These surreal requirements are one of the reasons the search for more efficient Transformers-like models has been on the rise.

An efficient design results in less training time, and less costs for training and inference in the form of processing units and energy, while allowing for even more powerful models or enabling new applications. One example of this is telecommunications and wireless, in which performance is a major factor in the relevancy of a technology. Another one is for medical tasks, in which on-device inference would solve many of the privacy-related concerns of the technology. 

A multitude of techniques has been applied to obtain efficient Transformer models, such as model compression, sparsification of what is called the "attention matrix", or memory mechanisms to process sub-sequences separately. While many of these can be successfully applied to some scenarios, during this report we will focus on two promising approaches: Quantization and Linear Attention.

Quantization is a form of model compression that represents model weights and intermediate features of the model with fewer bits. For example, as we mentioned above, LLaMa2-70B is commonly used with weights in float16 format rather than full precision (float32), which reduces half the memory footprint of the model. Most quantization approaches focus on more aggressive compression, with $8, 4,$ or $1$ bit integers for weights and/or activations of the model. In that way not only the memory requirements are drastically reduced, but also more efficient operations are used with integer operands. In this report, we will focus on approaches pushing for binary quantization, i.e. $1-$bit representations, due to the promise of a $32$x reduction in storage and the elimination of expensive multiplication in favor of bit-wise operations. Notably, binary neural networks are claimed to provide energy consumption improvements of between $100-1000$x over full precision counterparts \cite{energy7929192}, further motivating the exploration of this approach.

The Linear Attention method for Transformers consists of a kernel-based formulation of the attention mechanism of Transformers. As we will develop in section \ref{sec:transformers}, the global scope of the attention mechanism results in a quadratic complexity in computations and memory consumption with relation to the sequence length, which is one of the main bottlenecks of the original formulation. To tackle this, linear attention uses feature maps and associativity of matrix products to calculate an alternative formulation of the module.