\section{Introduction}

Transformers \cite{vaswani2023attention} was introduced in 2017 and since then has powered many of the recent advancements in Artificial Intelligence (AI). Inspired by the attention mechanism for LSTMs \cite{10.1162/neco.1997.9.8.1735}\cite{bahdanau2016neural}\cite{luong2015effective}\cite{kim2017structured} that was ubiquitous for sequence-to-sequence (seq-to-seq) tasks,  Vaswani et al. proposed a model architecture that removed the recurrent network often associated with it, and relied entirely on attention for the sequence modelling. In doing so, the Transformers architecture deals with one of the fundamental bottlenecks of recurrent networks, the limited parallelization during training due to the sequential nature of dependencies. On the other hand, the Transformer can be efficiently parallelized which allows not only for faster training, but also for deeper networks with more representational power. 

Transformers has been widely adopted in diverse fields,
such as natural language processing (NLP) and computer vision (CV), achieving state-of-the-art on various tasks. As such, there is a rich variety of research dedicated to applications and fundamental improvements to it's original formulation.